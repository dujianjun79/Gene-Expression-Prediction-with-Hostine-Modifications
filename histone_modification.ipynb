{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Gene Expression from Histone Modification\n",
    "\n",
    "Presenter: Jianjun Du\n",
    "\n",
    "Department of Computer Science, University of Texas at Dallas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "<p>Basically, this is a classification problem. Histone modifications are important factors controlling gene expression. In this data set, five histone modifications were chosen as features to predict the gene expression level</p>\n",
    "<p>The distinct characteristic is the combinatorial effect between different histone modifications.</p>\n",
    "<p>The following classification algorithms will be used:</p>\n",
    "<p>-----Support Vector Machine</p>\n",
    "<p>-----Logistic Regression</p>\n",
    "<p>-----Random Forest</p>\n",
    "<p>-----Etreme gradient booster</p>\n",
    "<p>-----Multiple layer neural network</p>\n",
    "<p>-----Convolutional neural network</p>\n",
    "<p></p>\n",
    "<p>Especially I will introduce the principles of CNN</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>dataset exploring</h1>\n",
    "<p>There are 15485 genes in the trainning dataset, and 100 bins and 5 diffrent histone modifications</p>\n",
    "<p>This will generate 500 features for this classification problems.</p> \n",
    "<p>The predicted class is the gene expression level for each gene</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many null values 0\n",
      "The manipulated data is:\n",
      "[[2 1 4 ..., 0 0 1]\n",
      " [1 0 1 ..., 0 0 0]\n",
      " [1 6 3 ..., 4 3 0]\n",
      " ..., \n",
      " [0 2 1 ..., 2 0 1]\n",
      " [0 0 0 ..., 2 0 0]\n",
      " [2 0 0 ..., 0 2 1]]\n",
      "data shape:  (15485, 500)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('x_train.csv') # read the train data from the file\n",
    "\n",
    "print(\"how many null values\", df.isnull().values.sum()) #count the numbers of missing value in the data\n",
    "\n",
    "data=df.iloc[:,1:6] # get rid of the GeneId column\n",
    "mydata=data.values   #convert the data frame to numpy array\n",
    "mydata=mydata.flatten() #convert the n-dimension array to one-dimension array\n",
    "\n",
    "X=np.reshape(mydata,(15485,500)) #reshape the array so that there are 500 columns each row\n",
    "\n",
    "print(\"The manipulated data is:\")\n",
    "print(X)\n",
    "print(\"data shape: \",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "robust_scaler=preprocessing.RobustScaler()\n",
    "x_train=robust_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ..., 0 0 0]\n",
      "(15485,)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('y_train.csv')\n",
    "df=df.iloc[:,1:2]\n",
    "y_train=df.values\n",
    "y=y_train.flatten()\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "# different models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This function is used for evaluate the best parameters for a model with cross validation</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_parameters(X_train,y_train,model,tuned_parameters):\n",
    "    \n",
    "    scores = ['accuracy']\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(model, tuned_parameters, cv=5,\n",
    "                           scoring='%s' % score)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 1000, 'tol': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.821 (+/-0.018) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'max_iter': 1000, 'tol': 0.001}\n",
      "0.816 (+/-0.008) for {'activation': 'relu', 'hidden_layer_sizes': (10, 20, 5), 'max_iter': 1000, 'tol': 0.001}\n",
      "0.839 (+/-0.018) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'max_iter': 1000, 'tol': 0.001}\n",
      "0.833 (+/-0.010) for {'activation': 'logistic', 'hidden_layer_sizes': (10, 20, 5), 'max_iter': 1000, 'tol': 0.001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "\n",
    "tuned_parameters=[{'hidden_layer_sizes':[(100,),(10,20,5)],\\\n",
    "                   'activation':['relu','logistic'],'tol':[0.001],'max_iter':[1000]}]\n",
    "\n",
    "model_parameters(x_train,y_train,MLPClassifier(),tuned_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'penalty': 'l1'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.838 (+/-0.019) for {'C': 200, 'penalty': 'l1'}\n",
      "0.838 (+/-0.020) for {'C': 200, 'penalty': 'l2'}\n",
      "0.837 (+/-0.019) for {'C': 150, 'penalty': 'l1'}\n",
      "0.837 (+/-0.019) for {'C': 150, 'penalty': 'l2'}\n",
      "0.838 (+/-0.019) for {'C': 100, 'penalty': 'l1'}\n",
      "0.837 (+/-0.019) for {'C': 100, 'penalty': 'l2'}\n",
      "0.838 (+/-0.019) for {'C': 20, 'penalty': 'l1'}\n",
      "0.838 (+/-0.019) for {'C': 20, 'penalty': 'l2'}\n",
      "0.839 (+/-0.020) for {'C': 1, 'penalty': 'l1'}\n",
      "0.838 (+/-0.019) for {'C': 1, 'penalty': 'l2'}\n",
      "0.839 (+/-0.019) for {'C': 0.5, 'penalty': 'l1'}\n",
      "0.838 (+/-0.019) for {'C': 0.5, 'penalty': 'l2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "tuned_parameters=[{'penalty':['l1','l2'],'C':[200,150,100,20,1,0.5]}]\n",
    "\n",
    "model_parameters(x_train,y_train,LogisticRegression(),tuned_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84665052,  0.84584342,  0.84140436,  0.8393866 ,  0.83077544])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='accuracy') # using preprocessed data\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84665052,  0.84584342,  0.84140436,  0.8393866 ,  0.83077544])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='accuracy') # using raw data\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8377724 ,  0.87409201,  0.84665052,  0.82728006,  0.84745763,\n",
       "        0.84180791,  0.84584342,  0.83292978,  0.81598063,  0.83993533])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "scores = cross_val_score(logreg,x_train,y_train,cv=10, scoring='accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83414044,  0.84221146,  0.83535109,  0.82808717,  0.82310178])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100, 2), random_state=1)\n",
    "scores = cross_val_score(clf,x_train,y_train,cv=5,scoring='accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier1 = linear_model.LogisticRegression(penalty='l1',C=0.5)\n",
    "classifier2 = SVC(kernel='linear', probability=True, random_state=1)\n",
    "classifier3 = RandomForestClassifier(random_state=1)\n",
    "classifier4 = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8927733220768258,\n",
       " 0.88522348857933497,\n",
       " 0.89060081609680586,\n",
       " 0.88299235495520845,\n",
       " 0.8785141409877586,\n",
       " 0.87347859840261055]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs = []\n",
    "for train, test in cv.split(x_train, y_train):\n",
    "    probas_ = classifier4.fit(x_train[train], y_train[train]).predict_proba(x_train[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[test], probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "aucs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture4.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture5.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=data.values.reshape(15485,100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('y_train.csv')\n",
    "df=df.iloc[:,1:2]\n",
    "y=df.values\n",
    "y=y.flatten()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 12\n",
    "\n",
    "img_rows,img_cols=100,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (10374, 100, 5, 1)\n",
      "10374 train samples\n",
      "5111 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10374 samples, validate on 5111 samples\n",
      "Epoch 1/12\n",
      "10374/10374 [==============================] - 3s 337us/step - loss: 0.4763 - acc: 0.8113 - val_loss: 0.4019 - val_acc: 0.8366\n",
      "Epoch 2/12\n",
      "10374/10374 [==============================] - 2s 219us/step - loss: 0.4162 - acc: 0.8353 - val_loss: 0.3895 - val_acc: 0.8421\n",
      "Epoch 3/12\n",
      "10374/10374 [==============================] - 2s 220us/step - loss: 0.4042 - acc: 0.8418 - val_loss: 0.3970 - val_acc: 0.8429\n",
      "Epoch 4/12\n",
      "10374/10374 [==============================] - 2s 220us/step - loss: 0.3924 - acc: 0.8415 - val_loss: 0.3792 - val_acc: 0.8470\n",
      "Epoch 5/12\n",
      "10374/10374 [==============================] - 2s 220us/step - loss: 0.3880 - acc: 0.8458 - val_loss: 0.3815 - val_acc: 0.8499\n",
      "Epoch 6/12\n",
      "10374/10374 [==============================] - 2s 220us/step - loss: 0.3844 - acc: 0.8472 - val_loss: 0.3919 - val_acc: 0.8484\n",
      "Epoch 7/12\n",
      "10374/10374 [==============================] - 2s 225us/step - loss: 0.3804 - acc: 0.8487 - val_loss: 0.3759 - val_acc: 0.8486\n",
      "Epoch 8/12\n",
      "10374/10374 [==============================] - 2s 225us/step - loss: 0.3772 - acc: 0.8514 - val_loss: 0.3772 - val_acc: 0.8472\n",
      "Epoch 9/12\n",
      "10374/10374 [==============================] - 2s 220us/step - loss: 0.3705 - acc: 0.8499 - val_loss: 0.3780 - val_acc: 0.8519\n",
      "Epoch 10/12\n",
      "10374/10374 [==============================] - 2s 225us/step - loss: 0.3728 - acc: 0.8520 - val_loss: 0.3845 - val_acc: 0.8507\n",
      "Epoch 11/12\n",
      "10374/10374 [==============================] - 2s 225us/step - loss: 0.3686 - acc: 0.8541 - val_loss: 0.3898 - val_acc: 0.8437\n",
      "Epoch 12/12\n",
      "10374/10374 [==============================] - 2s 236us/step - loss: 0.3670 - acc: 0.8541 - val_loss: 0.3861 - val_acc: 0.8515\n",
      "Test loss: 0.38605293975\n",
      "Test accuracy: 0.851496771716\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv2D(32, kernel_size=(2, 2),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
